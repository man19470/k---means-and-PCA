# -*- coding: utf-8 -*-
"""k means and PCA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HA3OVtXUgHeBTwz_w2q6GoJBe4U3cXKh
"""

!pip install pandas scikit-learn matplotlib seaborn

import pandas as pd

# Load data
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx"
data = pd.read_excel(url)

# Quick look
print(data.head())
print(data.shape)

# Drop missing values
data = data.dropna()

# Use numerical columns
data_numeric = data[['Quantity', 'UnitPrice']]

# Remove outliers
data_numeric = data_numeric[(data_numeric['Quantity'] > 0) & (data_numeric['UnitPrice'] > 0)]

# Quick check
print(data_numeric.head())
print(data_numeric.shape)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Small sample for speed
data_sample = data_numeric.sample(1000, random_state=42)

# Calculate inertia
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_sample)
    inertia.append(kmeans.inertia_)

# Plot Elbow curve
plt.plot(range(1, 11), inertia, marker='o')
plt.title("Elbow Method for Optimal K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.show()

# K-Means with chosen K (e.g., 3)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(data_sample)

# Add clusters
data_sample['Cluster'] = clusters

# Quick look
print(data_sample.head())
print("Cluster counts:", data_sample['Cluster'].value_counts())

from sklearn.decomposition import PCA

# Features for PCA
features = data_sample[['Quantity', 'UnitPrice']]

# Reduce to 2D
pca = PCA(n_components=2)
pca_result = pca.fit_transform(features)

# New dataframe
pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])
pca_df['Cluster'] = data_sample['Cluster']

# Quick look
print(pca_df.head())
print("Explained variance ratio:", pca.explained_variance_ratio_)

!pip install pandas scikit-learn matplotlib seaborn

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx"
data = pd.read_excel(url)

# Clean data
data = data.dropna()
data_numeric = data[['Quantity', 'UnitPrice']]
data_numeric = data_numeric[(data_numeric['Quantity'] > 0) & (data_numeric['UnitPrice'] > 0)]
data_sample = data_numeric.sample(1000, random_state=42)

# Elbow Method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_sample)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title("Elbow Method for Optimal K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.show()

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42)  # Adjust K
clusters = kmeans.fit_predict(data_sample)
data_sample['Cluster'] = clusters

# PCA
features = data_sample[['Quantity', 'UnitPrice']]
pca = PCA(n_components=2)
pca_result = pca.fit_transform(features)
pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])
pca_df['Cluster'] = clusters

# Visualize
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='deep')
plt.title("PCA of Online Retail Data with K-Means Clusters")
plt.show()

# Info
print("Cluster counts:", data_sample['Cluster'].value_counts())
print("Explained variance ratio:", pca.explained_variance_ratio_)

